<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>18.How to Run Ai locally on Your Pc :: Crusblog</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content=" Why Run AI Locally? Privacy: Your data stays on your machine. Cost: No API fees or subscriptions. Customization: Tweak models to solve your unique problems. Offline Access: Use AI even without an internet connection. Step 1: Installing Ollama Ollama works on Windows, macOS, and Linux. Here’s how to set it up:
For Windows (Preview): Download the Ollama Windows installer. Run the .exe file and follow the prompts. Open PowerShell or Command Prompt and test with ollama --version. For macOS/Linux: Open Terminal and run this command: curl -fsSL https://ollama.ai/install.sh | sh Verify the install: ollama --version. Step 2: Running Your First AI Model Ollama has a library of pre-trained models. Let’s start with Llama 2 (a popular open-source model by Meta):
" />
<meta name="keywords" content="Cybersecurity, AI, Gaming, Crusblog " />

  <meta name="robots" content="noodp" />

<link rel="canonical" href="/posts/blog-18-running-ai-on-your-local-pc-and-finetune-it/" />





  
  <link rel="stylesheet" href="/css/buttons.min.2bc533403a27dfe0e93105a92502b42ce4587e2e4a87d9f7d349e51e16e09478.css">

  
  <link rel="stylesheet" href="/css/code.min.00125962708925857e7b66dbc58391d55be1191a3d0ce2034de8c9cd2c481c36.css">

  
  <link rel="stylesheet" href="/css/fonts.min.4881f0c525f3ce2a1864fb6e96676396cebe1e6fcef1933e8e1dde7041004fb5.css">

  
  <link rel="stylesheet" href="/css/footer.min.2e3eb191baee58dd05a9f0104ac1fab0827bca7c64dafe0b2579f934c33a1d69.css">

  
  <link rel="stylesheet" href="/css/gist.min.a751e8b0abe1ba8bc53ced52a38b19d8950fe78ca29454ea8c2595cf26aad5c0.css">

  
  <link rel="stylesheet" href="/css/header.min.b6fb4423cf82a9f9d7abc9cd010223fa3d70a6526a3f28f8e17d814c06e18f9e.css">

  
  <link rel="stylesheet" href="/css/main.min.fe8dc560fccb53a458b0db19ccb7b265764ac46b68596b7e099c6793054dd457.css">

  
  <link rel="stylesheet" href="/css/menu.min.83637a90d903026bc280d3f82f96ceb06c5fc72b7c1a8d686afb5bbf818a29f7.css">

  
  <link rel="stylesheet" href="/css/pagination.min.82f6400eae7c7c6dc3c866733c2ec0579e4089608fea69400ff85b3880aa0d3c.css">

  
  <link rel="stylesheet" href="/css/post.min.fc74ca360273c1d828da3c02b8174eba435607b369d98418ccc6f2243cd4e75d.css">

  
  <link rel="stylesheet" href="/css/prism.min.9023bbc24533d09e97a51a0a42a5a7bfe4c591ae167c5551fb1d2191d11977c0.css">

  
  <link rel="stylesheet" href="/css/syntax.min.cc789ed9377260d7949ea4c18781fc58959a89287210fe4edbff44ebfc1511b6.css">

  
  <link rel="stylesheet" href="/css/terminal.min.dd0bf9c7cacb24c1b0184f52f1869b274e06689557468cc7030ccf632328eb97.css">

  
  <link rel="stylesheet" href="/css/terms.min.b81791663c3790e738e571cdbf802312390d30e4b1d8dc9d814a5b5454d0ac11.css">







<link rel="shortcut icon" href="/favicon.png">
<link rel="apple-touch-icon" href="/apple-touch-icon.png">


<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="18.How to Run Ai locally on Your Pc">
<meta property="og:description" content=" Why Run AI Locally? Privacy: Your data stays on your machine. Cost: No API fees or subscriptions. Customization: Tweak models to solve your unique problems. Offline Access: Use AI even without an internet connection. Step 1: Installing Ollama Ollama works on Windows, macOS, and Linux. Here’s how to set it up:
For Windows (Preview): Download the Ollama Windows installer. Run the .exe file and follow the prompts. Open PowerShell or Command Prompt and test with ollama --version. For macOS/Linux: Open Terminal and run this command: curl -fsSL https://ollama.ai/install.sh | sh Verify the install: ollama --version. Step 2: Running Your First AI Model Ollama has a library of pre-trained models. Let’s start with Llama 2 (a popular open-source model by Meta):
" />
<meta property="og:url" content="/posts/blog-18-running-ai-on-your-local-pc-and-finetune-it/" />
<meta property="og:site_name" content="Crusblog" />

  <meta property="og:image" content="/og-image.png">

<meta property="og:image:width" content="1200">
<meta property="og:image:height" content="627">


  <meta property="article:published_time" content="2025-01-25 00:00:00 &#43;0000 UTC" />












</head>
<body>


<div class="container">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Crusblog
  </div>
</a>

    </div>
    
      <ul class="menu menu--mobile">
  <li class="menu__trigger">Menu&nbsp;▾</li>
  <li>
    <ul class="menu__dropdown">
      
        
          <li><a href="/">Home</a></li>
        
      
        
          <li><a href="/titles">Blogs</a></li>
        
      
        
          <li><a href="/tags">Tags</a></li>
        
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/showcase">Showcase</a></li>
        
      
      
    </ul>
  </li>
</ul>

    
    
  </div>
  
    <nav class="navigation-menu">
  <ul class="navigation-menu__inner menu--desktop">
    
      
        
          <li><a href="/" >Home</a></li>
        
      
        
          <li><a href="/titles" >Blogs</a></li>
        
      
        
          <li><a href="/tags" >Tags</a></li>
        
      
        
          <li><a href="/about" >About</a></li>
        
      
        
          <li><a href="/showcase" >Showcase</a></li>
        
      
      
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<article class="post">
  <h1 class="post-title">
    <a href="/posts/blog-18-running-ai-on-your-local-pc-and-finetune-it/">18.How to Run Ai locally on Your Pc</a>
  </h1>
  <div class="post-meta"><time class="post-date">2025-01-25</time><span class="post-reading-time">4 min read (831 words)</span></div>

  
    <span class="post-tags">
      
      #<a href="/tags/aiml/">AIML</a>&nbsp;
      
      #<a href="/tags/basic/">basic</a>&nbsp;
      
      #<a href="/tags/setup/">setup</a>&nbsp;
      
    </span>
  
  


  
  
    <div class="table-of-contents">
      <h2>
        Table of Contents
      </h2>
      <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#why-run-ai-locally"><strong>Why Run AI Locally?</strong></a></li>
        <li><a href="#step-1-installing-ollama"><strong>Step 1: Installing Ollama</strong></a></li>
        <li><a href="#step-2-running-your-first-ai-model"><strong>Step 2: Running Your First AI Model</strong></a></li>
        <li><a href="#step-3-fine-tuning-your-model"><strong>Step 3: Fine-Tuning Your Model</strong></a></li>
        <li><a href="#tips-for-effective-fine-tuning"><strong>Tips for Effective Fine-Tuning</strong></a></li>
        <li><a href="#advanced-integrate-ollama-with-other-tools"><strong>Advanced: Integrate Ollama with Other Tools</strong></a></li>
        <li><a href="#running-deepseek-r1-locally-with-ollama"><strong>Running DeepSeek-R1 Locally with Ollama</strong></a></li>
        <li><a href="#step-2-updated-running-models-like-deepseek-r1"><strong>Step 2 (Updated): Running Models like DeepSeek-R1</strong></a></li>
        <li><a href="#step-3-updated-fine-tuning-deepseek-r1"><strong>Step 3 (Updated): Fine-Tuning DeepSeek-R1</strong></a></li>
        <li><a href="#tips-for-fine-tuning-deepseek-r1"><strong>Tips for Fine-Tuning DeepSeek-R1</strong></a></li>
        <li><a href="#advanced-use-cases-for-deepseek-r1"><strong>Advanced Use Cases for DeepSeek-R1</strong></a></li>
        <li><a href="#performance-notes"><strong>Performance Notes</strong></a></li>
        <li><a href="#final-thoughts"><strong>Final Thoughts</strong></a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
  

  <div class="post-content"><div>
        <hr>
<h3 id="why-run-ai-locally"><strong>Why Run AI Locally?</strong><a href="#why-run-ai-locally" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ol>
<li><strong>Privacy</strong>: Your data stays on your machine.</li>
<li><strong>Cost</strong>: No API fees or subscriptions.</li>
<li><strong>Customization</strong>: Tweak models to solve your unique problems.</li>
<li><strong>Offline Access</strong>: Use AI even without an internet connection.</li>
</ol>
<hr>
<h3 id="step-1-installing-ollama"><strong>Step 1: Installing Ollama</strong><a href="#step-1-installing-ollama" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>Ollama works on Windows, macOS, and Linux. Here’s how to set it up:</p>
<h4 id="for-windows-preview"><strong>For Windows (Preview)</strong>:<a href="#for-windows-preview" class="hanchor" ariaLabel="Anchor">#</a> </h4>
<ol>
<li>Download the <a href="https://ollama.com/download">Ollama Windows installer</a>.</li>
<li>Run the <code>.exe</code> file and follow the prompts.</li>
<li>Open PowerShell or Command Prompt and test with <code>ollama --version</code>.</li>
</ol>
<h4 id="for-macoslinux"><strong>For macOS/Linux</strong>:<a href="#for-macoslinux" class="hanchor" ariaLabel="Anchor">#</a> </h4>
<ol>
<li>Open Terminal and run this command:
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>curl -fsSL https://ollama.ai/install.sh | sh  
</span></span></code></pre></div></li>
<li>Verify the install: <code>ollama --version</code>.</li>
</ol>
<hr>
<h3 id="step-2-running-your-first-ai-model"><strong>Step 2: Running Your First AI Model</strong><a href="#step-2-running-your-first-ai-model" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>Ollama has a library of pre-trained models. Let’s start with <strong>Llama 2</strong> (a popular open-source model by Meta):</p>
<ol>
<li>
<p>In your terminal, run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run llama2  
</span></span></code></pre></div><p><em>(This downloads the model—it’s ~4GB, so grab a coffee.)</em></p>
</li>
<li>
<p><strong>Chat with the model</strong>:</p>
<pre tabindex="0"><code>&gt;&gt;&gt; Write a haiku about pizza  
Crispy crust whispers,  
Melting cheese hugs savory dreams—  
Slice of heaven’s warmth.  
</code></pre></li>
</ol>
<p><strong>Other models to try</strong>:</p>
<ul>
<li><code>mistral</code>: Fast and lightweight.</li>
<li><code>codellama</code>: Specializes in code generation.</li>
<li><code>phi3</code>: Microsoft’s small but powerful model.</li>
</ul>
<hr>
<h3 id="step-3-fine-tuning-your-model"><strong>Step 3: Fine-Tuning Your Model</strong><a href="#step-3-fine-tuning-your-model" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>Fine-tuning lets you adapt a model to your specific tasks. For example, you could train it to:</p>
<ul>
<li>Write in your brand’s voice.</li>
<li>Summarize medical reports.</li>
<li>Generate Python code for data analysis.</li>
</ul>
<h4 id="how-to-fine-tune-with-ollama"><strong>How to Fine-Tune with Ollama</strong>:<a href="#how-to-fine-tune-with-ollama" class="hanchor" ariaLabel="Anchor">#</a> </h4>
<ol>
<li>
<p><strong>Prepare Your Data</strong>:<br>
Create a <code>.txt</code> file with examples. For instance, if training a story-writing AI:</p>
<pre tabindex="0"><code>[Prompt]: Write a fantasy story about a robot knight  
[Response]: Sir Clank-a-Lot, a rusted but valiant robot, embarked on a quest to…  
</code></pre></li>
<li>
<p><strong>Create a Modelfile</strong>:<br>
This configures the base model and your training data. Save this as <code>modelfile.txt</code>:</p>
<pre tabindex="0"><code>FROM llama2  
SYSTEM &#34;&#34;&#34;You are a creative fantasy writer.&#34;&#34;&#34;  
MESSAGE user &#34;Write a story&#34;  
MESSAGE assistant &#34;Sir Clank-a-Lot...&#34;  
# Add more examples here  
</code></pre></li>
<li>
<p><strong>Train the Model</strong>:<br>
Run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama create my-custom-model -f modelfile.txt  
</span></span></code></pre></div></li>
<li>
<p><strong>Use Your Custom Model</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run my-custom-model  
</span></span></code></pre></div></li>
</ol>
<hr>
<h3 id="tips-for-effective-fine-tuning"><strong>Tips for Effective Fine-Tuning</strong><a href="#tips-for-effective-fine-tuning" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>Start Small</strong>: Fine-tune with 10-20 examples first.</li>
<li><strong>Quality Over Quantity</strong>: Use clear, diverse prompts/responses.</li>
<li><strong>Iterate</strong>: Test the model, find weaknesses, and add more data.</li>
</ul>
<hr>
<h3 id="advanced-integrate-ollama-with-other-tools"><strong>Advanced: Integrate Ollama with Other Tools</strong><a href="#advanced-integrate-ollama-with-other-tools" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>LangChain</strong>: Build AI workflows (e.g., connect Ollama to a PDF parser).</li>
<li><strong>Docker</strong>: Containerize your models for deployment.</li>
<li><strong>Ollama API</strong>: Use <code>http://localhost:11434</code> to integrate with apps like Obsidian or VS Code.</li>
</ul>
<hr>
<h3 id="running-deepseek-r1-locally-with-ollama"><strong>Running DeepSeek-R1 Locally with Ollama</strong><a href="#running-deepseek-r1-locally-with-ollama" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>DeepSeek-R1 is a standout model for technical tasks like code generation, debugging, and mathematical problem-solving. Here’s how to run and fine-tune it using Ollama:</p>
<hr>
<h3 id="step-2-updated-running-models-like-deepseek-r1"><strong>Step 2 (Updated): Running Models like DeepSeek-R1</strong><a href="#step-2-updated-running-models-like-deepseek-r1" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>After installing Ollama, you can pull and run DeepSeek-R1 with ease:</p>
<ol>
<li>
<p><strong>Download DeepSeek-R1</strong>:<br>
Open your terminal and run:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run deepseek-r1  
</span></span></code></pre></div><p><em>Note:</em> If the model isn’t listed publicly yet, you might need to pull it directly using its full name (e.g., <code>ollama run deepseek-ai/deepseek-r1</code>). Check the <a href="https://ollama.ai/library">Ollama model library</a> for exact syntax.</p>
</li>
<li>
<p><strong>Test Its Coding Skills</strong>:</p>
<pre tabindex="0"><code>&gt;&gt;&gt; Write a Python function to calculate Fibonacci numbers recursively  
</code></pre><p>The model should generate:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fibonacci</span>(n):  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> n <span style="color:#f92672">&lt;=</span> <span style="color:#ae81ff">1</span>:  
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> n  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:  
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> fibonacci(n<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>) <span style="color:#f92672">+</span> fibonacci(n<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>)  
</span></span></code></pre></div></li>
</ol>
<p><strong>Why DeepSeek-R1?</strong></p>
<ul>
<li>Excels at <strong>code generation</strong> (Python, JavaScript, etc.).</li>
<li>Strong at <strong>math/logic problems</strong> (e.g., SAT questions, algebra).</li>
<li>Compact size compared to giants like GPT-4, making it ideal for local use.</li>
</ul>
<hr>
<h3 id="step-3-updated-fine-tuning-deepseek-r1"><strong>Step 3 (Updated): Fine-Tuning DeepSeek-R1</strong><a href="#step-3-updated-fine-tuning-deepseek-r1" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>Let’s say you want to specialize DeepSeek-R1 for your company’s internal APIs or a niche programming language. Here’s how:</p>
<h4 id="example-training-it-for-internal-code-conventions"><strong>Example: Training It for Internal Code Conventions</strong><a href="#example-training-it-for-internal-code-conventions" class="hanchor" ariaLabel="Anchor">#</a> </h4>
<ol>
<li>
<p><strong>Prepare a Dataset</strong>:<br>
Create a <code>deepseek-data.txt</code> file with examples of code snippets paired with prompts:</p>
<pre tabindex="0"><code>[Prompt]: Write a Python function to connect to our internal database API  
[Response]: def connect_db(api_key):  
                from internal_db import Client  
                return Client(api_key, timeout=30)  
</code></pre></li>
<li>
<p><strong>Create a Modelfile</strong>:<br>
Save this as <code>deepseek-modelfile.txt</code>:</p>
<pre tabindex="0"><code>FROM deepseek-r1  
SYSTEM &#34;&#34;&#34;You are a senior Python developer for Acme Corp. Follow PEP8 and use internal libraries.&#34;&#34;&#34;  
MESSAGE user &#34;Write a Python function to connect to our internal database API&#34;  
MESSAGE assistant &#34;def connect_db(api_key):...&#34;  
# Add more coding examples  
</code></pre></li>
<li>
<p><strong>Train Your Custom Model</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama create acme-coder -f deepseek-modelfile.txt  
</span></span></code></pre></div></li>
<li>
<p><strong>Run and Test</strong>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>ollama run acme-coder  
</span></span><span style="display:flex;"><span>&gt;&gt;&gt; How <span style="color:#66d9ef">do</span> I fetch user data from the database?  
</span></span></code></pre></div><p>The model should now generate code using your company’s conventions.</p>
</li>
</ol>
<hr>
<h3 id="tips-for-fine-tuning-deepseek-r1"><strong>Tips for Fine-Tuning DeepSeek-R1</strong><a href="#tips-for-fine-tuning-deepseek-r1" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>Focus on Code Structure</strong>: Provide examples with clear input/output patterns.</li>
<li><strong>Include Error Handling</strong>: Train it to handle edge cases (e.g., <code>try/except</code> blocks).</li>
<li><strong>Use Small Batches</strong>: Start with 10-15 high-quality code examples to avoid overload.</li>
</ul>
<hr>
<h3 id="advanced-use-cases-for-deepseek-r1"><strong>Advanced Use Cases for DeepSeek-R1</strong><a href="#advanced-use-cases-for-deepseek-r1" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ol>
<li><strong>Documentation Generation</strong>:<br>
Fine-tune it to turn code comments into Markdown docs.</li>
<li><strong>Math Tutoring</strong>:<br>
Train it to solve and explain calculus problems step-by-step.</li>
<li><strong>CI/CD Automation</strong>:<br>
Integrate with GitHub Actions to review pull requests locally.</li>
</ol>
<hr>
<h3 id="performance-notes"><strong>Performance Notes</strong><a href="#performance-notes" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<ul>
<li><strong>Hardware Requirements</strong>: DeepSeek-R1 runs well on 16GB RAM, but GPU acceleration (e.g., NVIDIA) speeds up inference.</li>
<li><strong>Quantized Versions</strong>: Look for <code>deepseek-r1:4b-q4</code> for lighter-weight usage on low-end machines.</li>
</ul>
<hr>
<h3 id="final-thoughts"><strong>Final Thoughts</strong><a href="#final-thoughts" class="hanchor" ariaLabel="Anchor">#</a> </h3>
<p>DeepSeek-R1 turns your local machine into a coding powerhouse. Whether you’re automating workflows, tutoring yourself in math, or enforcing code standards, Ollama’s simplicity lets you experiment without cloud costs.</p>
<p><strong>Pro Tip</strong>: Combine DeepSeek-R1 with Ollama’s API (<code>http://localhost:11434</code>) and tools like <strong>VS Code</strong> or <strong>PyCharm</strong> for a seamless coding assistant.</p>
<hr>
<p>[Crusveder]</p>

      </div></div>

  
    
<div class="pagination">
  <div class="pagination__title">
    <span class="pagination__title-h">Read other posts</span>
    <hr />
  </div>
  <div class="pagination__buttons">
    
    
    
      <a href="/posts/blog-17-network-vulnerability-scanner/" class="button inline next">
        17. Developing a Vulnerability Scanner
      </a>
    
  </div>
</div>


  

  
    







  
</article>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2025 Powered by <a href="https://github.com/CRUSVEDER">Crusveder</a></span>
    
     
      </div>
  </div>
</footer>






<script type="text/javascript" src="/bundle.min.js"></script>





  
</div>

</body>
</html>
